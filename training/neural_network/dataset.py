"""
Dataset loader for GA-generated trajectory data.

This module implements a PyTorch Dataset for loading and preprocessing navigation
trajectories generated by the genetic algorithm. The dataset supports:
- Loading from pickle files
- Filtering low-quality trajectories by fitness percentile
- Train/validation splitting
- Data transformation for PyTorch model input

Expected data format (pickle file):
    List of trajectory dictionaries:
    [
        {
            'costmap': np.ndarray,          # (50, 50), float32, [0,1]
            'robot_state': np.ndarray,      # (9,), [x,y,theta,v_x,v_y,omega,a_x,a_y,alpha]
            'goal_relative': np.ndarray,    # (3,), [dx,dy,dtheta] in robot frame
            'costmap_metadata': np.ndarray, # (2,), [inflation_decay, resolution]
            'control_sequence': np.ndarray, # (20, 3), [[v_x,v_y,omega], ...]
            'fitness': float                # GA fitness score
        },
        ...
    ]
"""

import pickle
import numpy as np
import torch
from torch.utils.data import Dataset, Subset
from typing import Tuple, List


class TrajectoryDataset(Dataset):
    """
    PyTorch Dataset for GA-generated navigation trajectories.

    This dataset loads trajectory data from a pickle file, filters out low-quality
    trajectories based on fitness percentile, and provides samples in the format
    expected by the PlannerPolicy model.
    """

    def __init__(self, data_path: str, filter_percentile: int = 25, transform=None):
        """
        Initialize TrajectoryDataset.

        Args:
            data_path: Path to pickle file containing trajectory data
            filter_percentile: Remove bottom N% of trajectories by fitness (0-100)
            transform: Optional transform to apply to samples (not used in core implementation)
        """
        self.data_path = data_path
        self.filter_percentile = filter_percentile
        self.transform = transform

        # Load and filter data
        self.trajectories = self._load_and_filter_data()

        print(f"TrajectoryDataset initialized:")
        print(f"  Data path: {data_path}")
        print(f"  Filter percentile: {filter_percentile}%")
        print(f"  Dataset size: {len(self.trajectories)}")

    def _load_and_filter_data(self) -> List[dict]:
        """
        Load trajectory data from pickle and filter by fitness.

        Returns:
            filtered_trajectories: List of trajectory dicts passing fitness threshold
        """
        # Load pickle file
        with open(self.data_path, 'rb') as f:
            all_trajectories = pickle.load(f)

        if not isinstance(all_trajectories, list):
            raise ValueError(f"Expected list of trajectories, got {type(all_trajectories)}")

        if len(all_trajectories) == 0:
            raise ValueError("Loaded trajectory list is empty")

        # Extract fitness values
        fitness_values = [traj['fitness'] for traj in all_trajectories]

        # Calculate fitness threshold
        if self.filter_percentile > 0:
            fitness_threshold = np.percentile(fitness_values, self.filter_percentile)
            filtered_trajectories = [
                traj for traj in all_trajectories
                if traj['fitness'] >= fitness_threshold
            ]
            print(f"  Filtered {len(all_trajectories) - len(filtered_trajectories)} "
                  f"low-fitness trajectories (threshold: {fitness_threshold:.4f})")
        else:
            filtered_trajectories = all_trajectories

        if len(filtered_trajectories) == 0:
            raise ValueError("No trajectories remaining after filtering")

        return filtered_trajectories

    def __len__(self) -> int:
        """
        Get dataset size.

        Returns:
            length: Number of trajectories in dataset
        """
        return len(self.trajectories)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Get a single trajectory sample.

        Args:
            idx: Index of trajectory to retrieve

        Returns:
            Tuple of tensors:
            - costmap: [1, 50, 50] - normalized costmap with channel dimension
            - robot_state: [9] - robot state vector
            - goal_relative: [3] - relative goal position
            - costmap_metadata: [2] - costmap metadata
            - control_sequence: [60] - flattened control sequence
        """
        traj = self.trajectories[idx]

        # Extract data
        costmap = traj['costmap']  # (50, 50)
        robot_state = traj['robot_state']  # (9,)
        goal_relative = traj['goal_relative']  # (3,)
        costmap_metadata = traj['costmap_metadata']  # (2,)
        control_sequence = traj['control_sequence']  # (20, 3)

        # Validate shapes
        assert costmap.shape == (50, 50), f"Expected costmap shape (50, 50), got {costmap.shape}"
        assert robot_state.shape == (9,), f"Expected robot_state shape (9,), got {robot_state.shape}"
        assert goal_relative.shape == (3,), f"Expected goal_relative shape (3,), got {goal_relative.shape}"
        assert costmap_metadata.shape == (2,), f"Expected costmap_metadata shape (2,), got {costmap_metadata.shape}"
        assert control_sequence.shape == (20, 3), f"Expected control_sequence shape (20, 3), got {control_sequence.shape}"

        # Transform data
        # Add channel dimension to costmap: (50, 50) -> (1, 50, 50)
        costmap = np.expand_dims(costmap, axis=0)

        # Flatten control sequence: (20, 3) -> (60,)
        control_sequence = control_sequence.flatten()

        # Convert to PyTorch tensors (float32)
        costmap = torch.from_numpy(costmap).float()
        robot_state = torch.from_numpy(robot_state).float()
        goal_relative = torch.from_numpy(goal_relative).float()
        costmap_metadata = torch.from_numpy(costmap_metadata).float()
        control_sequence = torch.from_numpy(control_sequence).float()

        # Apply optional transform
        if self.transform is not None:
            costmap, robot_state, goal_relative, costmap_metadata, control_sequence = \
                self.transform(costmap, robot_state, goal_relative, costmap_metadata, control_sequence)

        return costmap, robot_state, goal_relative, costmap_metadata, control_sequence

    def train_test_split(self, train_ratio: float = 0.8, random_seed: int = 42) -> Tuple[Subset, Subset]:
        """
        Split dataset into training and validation subsets.

        Args:
            train_ratio: Fraction of data to use for training (0-1)
            random_seed: Random seed for reproducibility

        Returns:
            Tuple of (train_dataset, val_dataset) as Subset objects
        """
        if not 0 < train_ratio < 1:
            raise ValueError(f"train_ratio must be between 0 and 1, got {train_ratio}")

        # Set random seed for reproducibility
        np.random.seed(random_seed)

        # Generate random permutation of indices
        num_samples = len(self)
        indices = np.random.permutation(num_samples)

        # Split indices
        split_idx = int(num_samples * train_ratio)
        train_indices = indices[:split_idx]
        val_indices = indices[split_idx:]

        # Create subsets
        train_dataset = Subset(self, train_indices)
        val_dataset = Subset(self, val_indices)

        print(f"Dataset split (seed={random_seed}):")
        print(f"  Train: {len(train_dataset)} samples ({len(train_dataset) / num_samples * 100:.1f}%)")
        print(f"  Val: {len(val_dataset)} samples ({len(val_dataset) / num_samples * 100:.1f}%)")

        return train_dataset, val_dataset

    def get_statistics(self) -> dict:
        """
        Compute statistics of the dataset.

        Returns:
            stats: Dictionary containing dataset statistics
        """
        fitness_values = [traj['fitness'] for traj in self.trajectories]

        stats = {
            'num_samples': len(self),
            'fitness': {
                'mean': np.mean(fitness_values),
                'std': np.std(fitness_values),
                'min': np.min(fitness_values),
                'max': np.max(fitness_values),
                'median': np.median(fitness_values)
            }
        }

        return stats

    def print_statistics(self):
        """
        Print dataset statistics to console.
        """
        stats = self.get_statistics()

        print("\n" + "=" * 60)
        print("Dataset Statistics")
        print("=" * 60)
        print(f"Number of samples: {stats['num_samples']}")
        print(f"\nFitness distribution:")
        print(f"  Mean: {stats['fitness']['mean']:.4f}")
        print(f"  Std: {stats['fitness']['std']:.4f}")
        print(f"  Min: {stats['fitness']['min']:.4f}")
        print(f"  Max: {stats['fitness']['max']:.4f}")
        print(f"  Median: {stats['fitness']['median']:.4f}")
        print("=" * 60 + "\n")


def create_synthetic_data(num_samples: int = 100, output_path: str = None) -> List[dict]:
    """
    Create synthetic trajectory data for testing purposes.

    This function generates random but valid trajectory data matching the expected
    format. Useful for testing the dataset loader and model before GA training is complete.

    Args:
        num_samples: Number of synthetic trajectories to generate
        output_path: Optional path to save synthetic data as pickle file

    Returns:
        trajectories: List of synthetic trajectory dictionaries
    """
    trajectories = []

    for i in range(num_samples):
        # Generate random but valid data
        traj = {
            'costmap': np.random.rand(50, 50).astype(np.float32),  # Random costmap [0, 1]
            'robot_state': np.random.randn(9).astype(np.float32),  # Random state
            'goal_relative': np.random.randn(3).astype(np.float32),  # Random goal
            'costmap_metadata': np.array([0.8, 0.05], dtype=np.float32),  # Fixed metadata
            'control_sequence': np.random.uniform(-1, 1, size=(20, 3)).astype(np.float32),  # Random controls
            'fitness': np.random.uniform(0, 10)  # Random fitness
        }
        trajectories.append(traj)

    # Save to file if requested
    if output_path is not None:
        with open(output_path, 'wb') as f:
            pickle.dump(trajectories, f)
        print(f"Saved {num_samples} synthetic trajectories to {output_path}")

    return trajectories


if __name__ == "__main__":
    # Test dataset with synthetic data
    print("Testing TrajectoryDataset with synthetic data...\n")

    # Create synthetic data
    synthetic_path = "/tmp/synthetic_trajectories.pkl"
    create_synthetic_data(num_samples=100, output_path=synthetic_path)

    # Load dataset
    dataset = TrajectoryDataset(synthetic_path, filter_percentile=25)
    dataset.print_statistics()

    # Test __getitem__
    print("Testing __getitem__:")
    costmap, robot_state, goal_relative, costmap_metadata, control_sequence = dataset[0]
    print(f"  Costmap shape: {costmap.shape} (expected: torch.Size([1, 50, 50]))")
    print(f"  Robot state shape: {robot_state.shape} (expected: torch.Size([9]))")
    print(f"  Goal relative shape: {goal_relative.shape} (expected: torch.Size([3]))")
    print(f"  Costmap metadata shape: {costmap_metadata.shape} (expected: torch.Size([2]))")
    print(f"  Control sequence shape: {control_sequence.shape} (expected: torch.Size([60]))")

    assert costmap.shape == torch.Size([1, 50, 50])
    assert robot_state.shape == torch.Size([9])
    assert goal_relative.shape == torch.Size([3])
    assert costmap_metadata.shape == torch.Size([2])
    assert control_sequence.shape == torch.Size([60])

    # Test train/val split
    print("\nTesting train_test_split:")
    train_dataset, val_dataset = dataset.train_test_split(train_ratio=0.8, random_seed=42)
    print(f"  Train size: {len(train_dataset)}")
    print(f"  Val size: {len(val_dataset)}")

    # Test dataloader
    print("\nTesting DataLoader:")
    from torch.utils.data import DataLoader

    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
    batch = next(iter(train_loader))
    costmap_batch, robot_state_batch, goal_batch, metadata_batch, control_batch = batch

    print(f"  Batch shapes:")
    print(f"    Costmap: {costmap_batch.shape} (expected: torch.Size([4, 1, 50, 50]))")
    print(f"    Robot state: {robot_state_batch.shape} (expected: torch.Size([4, 9]))")
    print(f"    Goal relative: {goal_batch.shape} (expected: torch.Size([4, 3]))")
    print(f"    Costmap metadata: {metadata_batch.shape} (expected: torch.Size([4, 2]))")
    print(f"    Control sequence: {control_batch.shape} (expected: torch.Size([4, 60]))")

    assert costmap_batch.shape == torch.Size([4, 1, 50, 50])
    assert robot_state_batch.shape == torch.Size([4, 9])
    assert goal_batch.shape == torch.Size([4, 3])
    assert metadata_batch.shape == torch.Size([4, 2])
    assert control_batch.shape == torch.Size([4, 60])

    print("\nâœ“ Dataset tests passed!")
